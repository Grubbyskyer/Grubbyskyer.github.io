<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Stanford Machine Learning Week3- -Logistic Regression and Regularization | Xing&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第三周讲了第一个分类算法- -逻辑回归，逻辑回归虽然名字里面包含了回归两个字，但其实它是一个分类算法。参考Wiki上对Logistic Regression的定义：

In statistics, logistic regression is a regression model where the dependent variable (DV) is categorical. —wiki 

本">
<meta property="og:type" content="article">
<meta property="og:title" content="Stanford Machine Learning Week3- -Logistic Regression and Regularization">
<meta property="og:url" content="http://yoursite.com/2015/10/26/Stanford Machine Learning Week3- -Logistic Regression and Regularization/index.html">
<meta property="og:site_name" content="Xing's Blog">
<meta property="og:description" content="第三周讲了第一个分类算法- -逻辑回归，逻辑回归虽然名字里面包含了回归两个字，但其实它是一个分类算法。参考Wiki上对Logistic Regression的定义：

In statistics, logistic regression is a regression model where the dependent variable (DV) is categorical. —wiki 

本">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-1.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-2.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-4.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-10.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-5.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-7.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-8.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-11.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-13.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-14.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-16.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-17.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-18.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-19.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-20.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-21.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-22.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-24.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-25.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-27.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-29.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-30.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-31.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-32.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-34.png">
<meta property="og:image" content="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-33.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stanford Machine Learning Week3- -Logistic Regression and Regularization">
<meta name="twitter:description" content="第三周讲了第一个分类算法- -逻辑回归，逻辑回归虽然名字里面包含了回归两个字，但其实它是一个分类算法。参考Wiki上对Logistic Regression的定义：

In statistics, logistic regression is a regression model where the dependent variable (DV) is categorical. —wiki 

本">
  
    <link rel="alternative" href="/atom.xml" title="Xing&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/author.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">TIAN Xing</a></h1>
		</hgroup>

		
		<p class="header-subtitle">人生的乐趣就在于永不休止地折腾、折腾、折腾...</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/instagram">相册</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/Grubbyskyer" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/234583516" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/tian-xing-60-64" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/JSP/" style="font-size: 10px;">JSP</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a><a href="/tags/hexo/" style="font-size: 15px;">hexo</a><a href="/tags/mysql/" style="font-size: 10px;">mysql</a><a href="/tags/python3/" style="font-size: 10px;">python3</a><a href="/tags/test/" style="font-size: 10px;">test</a><a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">中南大学10级，现在本校读研。研究机器学习和自然语言处理相关算法。直男癌O^·^O死♂宅O^·^O极客范</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">TIAN Xing</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/author.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">TIAN Xing</h1>
			</hgroup>
			
			<p class="header-subtitle">人生的乐趣就在于永不休止地折腾、折腾、折腾...</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/instagram">相册</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Grubbyskyer" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/234583516" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/tian-xing-60-64" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-Stanford Machine Learning Week3- -Logistic Regression and Regularization" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/10/26/Stanford Machine Learning Week3- -Logistic Regression and Regularization/" class="article-date">
  	<time datetime="2015-10-26T13:19:00.000Z" itemprop="datePublished">Oct 26</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Stanford Machine Learning Week3- -Logistic Regression and Regularization
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>第三周讲了第一个分类算法- -<code>逻辑回归</code>，逻辑回归虽然名字里面包含了回归两个字，但其实它是一个分类算法。参考Wiki上对Logistic Regression的定义：</p>
<blockquote>
<p>In statistics, logistic regression is a regression model where the dependent variable (DV) is categorical. —wiki </p>
</blockquote>
<p>本周课程从最基本的二元分类讲起，引入了<code>sigmoid</code>函数，逻辑回归的<code>cost function</code>和参数学习的方法，然后介绍了将二元分类拓展到多元分类的<code>one-vs-all</code>方法，最后针对模型存在的<code>过拟合(overfitting)</code>问题介绍了<code>正则化(regularization)</code>。<br><a id="more"></a></p>
<h2 id="Classification_and_Representation">Classification and Representation</h2><h3 id="Classification">Classification</h3><p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-1.png" alt=""><br>我们在日常生活中肯定遇见过这样的系统，判断是否是垃圾邮件、是否是网上诈骗、是否是恶性肿瘤，这里的因变量（或者叫做预测值）不再像线性回归中的房价预测模型中的因变量那样是连续的值，而是一些<strong>离散的值</strong>。<br>比如在上述的三个例子中，我们想得到的结果都只有是和否两个值，在数学上表示也就是 $y\in\{0, 1\}$。一般情况下，哪个用0表示，哪个用1表示是任意的(arbitrary)，但是习惯上，我们用0表示<code>negative class</code>，用1表示<code>positive class</code>，比如我们想判断一封邮件是不是垃圾邮件，则0表示不是(absence)垃圾邮件，1表示是(presence)垃圾邮件。<br>当然了，很多问题上结果不只是两类，比如邮箱的邮件自动分类系统，有家人、朋友、同学等等，此时$y\in\{0, 1, 2, \cdots,n\}$，后面会有一个算法来将二元分类拓展到多元分类上，现在先来看最基础的二元分类。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-2.png" alt=""><br>还是根据肿瘤大小预测癌症的例子，如果只有左边那8个点，我们像线性回归那样将它们画到坐标系中，然后应用线性回归，我们可能得到的是图中那条粉色斜线，在预测时，根据此模型算出 $h_\theta(x)$，如果 $h_\theta(x) \geqslant 0.5$，则预测为1，否则预测为0，只看x的话，也就是竖直蓝线右边预测为1，否则预测为0。这样看起来好像是正确的，但是其实这只是碰巧，我们在训练集中再加一个样本（最右边的那个点）。此时应用线性回归我们得到的模型可能就是图中那条靠右的蓝线，这个模型显然有很大的误差。</p>
<p>而且结果y只有0和1两个值，我们的模型如果得到 $h_\theta(x) &gt; 1$ 或者 $h_\theta(x) &lt; 0$ 总归是有点奇怪，我们需要一个模型，它预测出来的 $h_\theta(x)$ 应该在0~1之间。</p>
<h3 id="Hypothesis_Representation">Hypothesis Representation</h3><p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-4.png" alt=""><br>既然我们想要 $0 \leqslant h_\theta(x) \leqslant 1$，所以我们需要一个函数将输入值映射到0~1之间，这里就引入了<code>sigmoid</code>函数：<br>$$g(z) = \frac{1}{1 + e^{-z}}$$<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-10.png" alt=""><br>此时我们的模型 $h_\theta(x)$ 由 $h_\theta(x) = \theta^T x$ 变为了 $h_\theta(x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$ ，这样得到的值就在0~1之间了。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-5.png" alt=""><br>我们来看一下，得到的 $h_\theta(x)$ 的值的意义。实际上，这个值代表了给定x的条件下y=1的概率，数学表示就是：<br>$$h_\theta(x) = P(y = 1\mid x ; \theta)$$<br>注意公式中间用了<code>;</code>表示这里的 $\theta$ 是一个参数，而不是条件，也就是说这里用的是经典概率论而不是贝叶斯概率论。</p>
<h3 id="Decision_Boundary">Decision Boundary</h3><p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-7.png" alt=""><br>根据我们的模型，$h_\theta(x) \geqslant 0.5$ 则预测 $y = 1$，亦即 $\theta^T x \geqslant 0$，同样地，$h_\theta(x) &lt; 0.5$ 等价于 $\theta^T x &lt; 0$，那么 $\theta^T x = 0$ 这条线就是我们根据此模型得出的分界线，叫做<code>decision boundary</code>。除了线性的分界线，如果选择高次项作为特征项，可以得到非线性的分界线，如下图所示。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-8.png" alt=""></p>
<h2 id="Logistic_Regression_Model">Logistic Regression Model</h2><h3 id="Cost_Function">Cost Function</h3><p>有了模型之后我们如何定义逻辑回归的<code>cost function</code>呢？如果按照线性回归里那样定义为平方项之和，那么由于这里加了一重<code>sigmoid</code>函数，可以证明，这样得到的<code>cost function</code>将是<code>non-convex</code>，亦即<strong>非凸的</strong>，那么其将会有很多局部最小值，从而无法用梯度下降等算法求解，见下图左侧样图。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-11.png" alt=""><br>因此我们需要找到一个<strong>convex</strong>的函数，那么可以定义<code>Cost</code>为如下形式：<br>$$Cost(h_\theta(x), y) = \begin{cases}\begin{aligned}<br>-log(h_\theta(x))  \quad  if \, y \,&amp;= 1\\<br>-log(1 - h_\theta(x))  \quad  if \, y \,&amp;= 0<br>\end{aligned}<br>\end{cases}$$<br>可以用凸函数的定义证明，这样定义的<code>cost function</code>是满足凸性的。</p>
<h3 id="Simplified_Cost_Function_and_Gradient_Descent">Simplified Cost Function and Gradient Descent</h3><p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-13.png" alt=""><br>注意到上面<code>Cost</code>的形式，其中y只能取0或1，我们可以把它写成一个式子：<br>$$Cost(h_\theta(x),y) = -y \cdot log(h_\theta(x)) - (1-y) \cdot log(1 - h_\theta(x))$$<br>如果一时没有看明白的话，可以手动取y=1和y=0模拟一下，就会发现和上面分开写的结果是一样的。<br>这样我们的损失函数 $J(\theta)$ 就可以写为如下形式：<br>$$J(\theta) = -\frac{1}{m}\sum_{i = 1}^{m}(y^{(i)} \cdot log(h_\theta(x^{(i)})) + (1-y^{(i)}) \cdot log(1 - h_\theta(x^{(i)})))$$<br>接下来还是应用梯度下降算法去计算 $\theta$ 的值。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-14.png" alt=""><br>参数更新的过程和线性回归一样，也需要计算 $\frac{\partial J(\theta)}{\partial\theta_{j}}$，由于 $-\frac{1}{m}$ 和 $y^{(i)}$ 在对 $\theta_j$ 求偏导数时都是常数，所以我们只用看 $log(h_\theta(x^{(i)}))$ 和 $log(1 - h_\theta(x^{(i)}))$，注意到 $h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$，应用复合函数求导法则即可得结果。这里以前者为例：<br>$$\begin{aligned}<br>\frac{\partial}{\partial \theta_j} log(h_\theta(x)) \,&amp;= \frac{1}{h_\theta(x)}\cdot (h_\theta(x))’ \\<br>&amp;= \frac{1}{h_\theta(x)}\cdot \frac{-1}{(1 + e^{-\theta^T x})^2} \cdot (1+e^{-\theta^T x})’ \\<br>&amp;= \frac{1}{h_\theta(x)}\cdot \frac{-1}{(1 + e^{-\theta^T x})^2} \cdot (-x_j)\cdot e^{-\theta^T x}\\<br>&amp;= x_j \cdot \frac{e^{-\theta^T x}}{1 + e^{-\theta^T x}}\\<br>&amp;= x_j \cdot (1-h_\theta(x))<br>\end{aligned}$$<br>同样可得：<br>$$\frac{\partial}{\partial \theta_j} log(1-h_\theta(x)) \,= -x_j \cdot h_\theta(x)$$<br>将其带入到 $J(\theta)$ 里，可得：<br>$$\frac{\partial J(\theta)}{\partial\theta_{j}} = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}$$<br>那么 $\theta_j$ 的更新过程为：<br>$$\theta_j := \theta_j - \alpha \cdot  \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}$$<br>是否感觉这个式子很眼熟呢？好像和线性回归里面的式子一模一样啊，怎么回事，我们开头不是还说不能用线性回归吗？其实这两个式子只是形式上一样，但其是<strong>完全不同</strong>的。因为在线性回归中，$h_\theta(x) = \theta^T x$，而在逻辑回归中，$h_\theta(x) = sigmoid(\theta^T x)$。<br>还需要提一点的是，在逻辑回归中，<code>feature scaling</code>仍然是很有效的令迭代速度加快的方法。</p>
<h3 id="Advanced_Optimization">Advanced Optimization</h3><p>求解参数 $\theta$ 的方法除了梯度下降外，还有很多比较高级的方法，当然也更复杂，如下：<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-16.png" alt=""><br>共轭梯度法，拟牛顿法等等。它们的优点是：①不需要手动选择学习率；②比梯度下降更快（事实上，在最优化里讲过，这些高级的方法具有<strong>二次终止性</strong>，而梯度下降算法不具备此性质）。当然，缺点是比较复杂，自己实现的话容易出错。不过好在有库函数这个东西。。。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-17.png" alt=""><br>如上图所示，我们只需要在<code>costfunction</code>函数的编写过程中，顺便返回对各个参数的偏导数，然后调用库函数就行了，具体地，会在下面的编程作业中看到其应用。</p>
<h2 id="Multiclass_Classification">Multiclass Classification</h2><h3 id="Multiclass_Classification:_One-vs-all">Multiclass Classification: One-vs-all</h3><p>截止到目前，我们讨论的逻辑回归都只是两类的分类问题，那如何把它拓展到多类呢？有一个算法叫做<code>one-vs-all</code>或者<code>one-vs-rest</code><br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-18.png" alt=""><br>如上图三类的问题，我们需要训练3个模型，训练第一个模型时，把三角当作1，红叉和方框都当作0，训练出 $h_\theta^{(1)}(x)$，同理训练出 $h_\theta^{(2)}(x)$ 和 $h_\theta^{(3)}(x)$，考虑到 $h_\theta(x)$ 的概率意义：<br>$$h_\theta^{(i)}(x) = p(y=i \mid x;\theta)$$<br>所以对一个测试样例，我们算出每个模型预测的值，取其值最大者即为我们预测的结果。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-19.png" alt=""><br>对于两类的分类问题，我们只需要训练<strong>一个</strong>模型。<br>对于K(K &gt; 2)类的分类问题，需要训练<strong>K</strong>个模型。</p>
<h2 id="Regularization">Regularization</h2><h3 id="The_Problem_of_Overfitting">The Problem of Overfitting</h3><p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-20.png" alt=""><br>在上图的线性回归模型中，总共只有5个样本，我们分别选取1、2、4个特征建立了左中右3个模型。可以看到，左边那个模型，对于训练集的预测仍有较大的误差，这种情况叫做<code>underfit</code>或者<code>high bias</code>；中间那个模型，直观上可以看出是比较正确的模型；而右边的四次模型，虽然完美拟合了训练集中的5个样本，但是直观上可以明显知道它不是正确的，比如 size=0 时它预测的值明显是不对的。这种很好地拟合了训练集，但是在测试集上却表现很差的情况就叫做<code>overfit</code>或者<code>high variance</code>。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-21.png" alt="21"><br>这是一个逻辑回归中的例子。最右边的高次模型“丧心病狂地”拟合出了一条如此妖魔化的分界线。。。<br>那么如何解决过拟合的问题呢？<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-22.png" alt=""></p>
<ol>
<li>减少模型的特征数<br>①手动选择一部分特征<br>②模型选择算法，自动帮我们选择合适的特征（后面会讲）</li>
<li>正则化<code>regularization</code><br>①保留所有的特征，但是减小模型中参数的值<br>②因为每种特征都或多或少地包含了一定信息，所以保留所有特征能最大限度地利用所有信息</li>
</ol>
<h3 id="Cost_Function-1">Cost Function</h3><p>如何减小 $\theta$ 的值呢？我们可以利用最优化理论里面的罚方法，给 $\theta$ 增加比较大的惩罚项，这里还是加的二次方的惩罚项，即正则化的损失函数为：<br>$$J_{new}(\theta) = J_{old}(\theta) + \lambda\sum_{j=1}^{n}\theta_j^2$$</p>
<ol>
<li>新加入的惩罚项为凸函数，显然新的损失函数仍然满足凸性</li>
<li>$\lambda$ 叫做<code>regularization parameter</code>，它的值会影响模型的效果</li>
<li>注意这里并没有对 $\theta_0$ 添加惩罚，加上也可以</li>
<li>考虑新的损失函数，令其取最小值，显然各个参数的值要比原来小了</li>
</ol>
<h3 id="Regularized_Linear_Regression">Regularized Linear Regression</h3><p>线性回归中使用正则化，新的<code>cost function</code>为：<br>$$J(\theta) = \frac{1}{2m}\cdot [\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]$$<br>应用梯度下降算法：<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-24.png" alt=""><br>由于没有对 $\theta_0$ 添加惩罚，所以求偏导数时不变，而其他的参数更新过程变为：<br>$$\theta_j := \theta_j - \alpha \cdot  \frac{1}{m}[\sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} + \frac{\lambda}{m}\cdot \theta_j]$$<br>这里如果做一个小变换，右式提取出公因子 $\theta_j$，可写成如下形式：<br>$$\theta_j := \theta_j \cdot(1 - \alpha\cdot\frac{\lambda}{m}) - \alpha \cdot  \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}$$<br>由于 $\alpha$、$\lambda$、$m$ 都是正数，所以 $1 - \alpha\cdot\frac{\lambda}{m} &lt; 1$，这样可以直观地感受出来加入了正则化之后的参数确实比原来的小了，但其实它的本质还是最上面的式子- -偏导数。<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-25.png" alt=""><br>讲解线性回归时候，我们还有一种求参数的方法是正规方程组。参照上一讲的推导过程，这里 $J(\theta)$ 相比原来多了 $\theta^T \theta$，所以<br>$$\frac{\partial J}{\partial \theta} = 2X^T X \theta - 2X^T y + 2\theta$$<br>令其等于0，得<br> $$\theta = (X^T X + \lambda I)^{-1}X^T y$$<br> 其中 $I$ 为单位矩阵。<br> 如果不对 $\theta_0$ 进行惩罚的话，那么最终的结果就是上图中的形式。需要注意的是，被Ng用蓝括号括起来的方阵，数学上可以证明，是<strong>一定可逆</strong>的。</p>
<h3 id="Regularized_Logistic_Regression">Regularized Logistic Regression</h3><p>逻辑回归中使用正则化，新的<code>cost function</code>为：<br>$$J(\theta) = -\frac{1}{m}\sum_{i = 1}^{m}(y^{(i)} \cdot log(h_\theta(x^{(i)})) + (1-y^{(i)}) \cdot log(1 - h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\cdot\sum_{j=1}^{n}\theta_j^2$$<br>应用梯度下降算法的过程也与线性回归类似，见下图：<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-27.png" alt=""><br>需要注意的是，这里的 $h_\theta(x)$ 是 $sigmoid(\theta^T x)$。</p>
<h2 id="Assignment">Assignment</h2><p>这次的作业也分为两部分，一是<code>Logistic regression</code>，二是<code>regularization</code>。</p>
<h3 id="Logistic_Regression">Logistic Regression</h3><p>这次的例子是根据一个学生两次考试的成绩判断他是否能被大学接收。<br><code>加载数据</code><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = load(<span class="string">'ex2data1.txt'</span>);</span><br><span class="line">X = data(:, <span class="matrix">[<span class="number">1</span>, <span class="number">2</span>]</span>); y = data(:, <span class="number">3</span>);</span><br></pre></td></tr></table></figure></p>
<p><code>plotData</code>- - 画出数据集<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Find Indices of Positive and Negative Examples</span></span><br><span class="line">pos = <span class="built_in">find</span>(y==<span class="number">1</span>); neg = <span class="built_in">find</span>(y == <span class="number">0</span>);</span><br><span class="line"><span class="comment">% Plot Examples</span></span><br><span class="line">plot(X(pos, <span class="number">1</span>), X(pos, <span class="number">2</span>), <span class="string">'k+'</span>,<span class="string">'LineWidth'</span>, <span class="number">2</span>, ...</span><br><span class="line"><span class="string">'MarkerSize'</span>, <span class="number">7</span>);</span><br><span class="line">plot(X(neg, <span class="number">1</span>), X(neg, <span class="number">2</span>), <span class="string">'ko'</span>, <span class="string">'MarkerFaceColor'</span>, <span class="string">'y'</span>, ...</span><br><span class="line"><span class="string">'MarkerSize'</span>, <span class="number">7</span>);</span><br><span class="line"><span class="comment">% Labels and Legend</span></span><br><span class="line">xlabel(<span class="string">'Exam 1 score'</span>)</span><br><span class="line">ylabel(<span class="string">'Exam 2 score'</span>)</span><br><span class="line"><span class="comment">% Specified in plot order</span></span><br><span class="line">legend(<span class="string">'Admitted'</span>, <span class="string">'Not admitted'</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-29.png" alt=""><br>为X添加X0=1那一列<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="matrix">[m, n]</span> = <span class="built_in">size</span>(X);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Add intercept term to x and X_test</span></span><br><span class="line">X = <span class="matrix">[ones(m, <span class="number">1</span>) X]</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize fitting parameters</span></span><br><span class="line">initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</span><br></pre></td></tr></table></figure></p>
<p><code>sigmoid</code>- - sigmoid函数<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">g</span> = <span class="title">sigmoid</span><span class="params">(z)</span></span></span><br><span class="line"><span class="comment">%SIGMOID Compute sigmoid functoon</span></span><br><span class="line">g = <span class="built_in">zeros</span>(<span class="built_in">size</span>(z));</span><br><span class="line">g = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-z));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p><code>costfunction</code>- - 按照公式返回 $J(\theta)$ 和各个参数的偏导数<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly </span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    h = sigmoid(X(<span class="built_in">i</span>, :) * theta);</span><br><span class="line">    J = J + y(<span class="built_in">i</span>) * <span class="built_in">log</span>(h) + (<span class="number">1</span> - y(<span class="built_in">i</span>)) * <span class="built_in">log</span>(<span class="number">1</span> - h);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">J = (-<span class="number">1</span>/m) * J;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">size</span>(X, <span class="number">2</span>)</span><br><span class="line">    h = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">        h = h + (sigmoid(X(<span class="built_in">i</span>, :) * theta) - y(<span class="built_in">i</span>)) * X(<span class="built_in">i</span>, <span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line">    grad(<span class="built_in">j</span>) = h / m;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure></p>
<p><code>使用fminunc计算参数值</code><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Compute and display initial cost and gradient</span></span><br><span class="line"><span class="matrix">[cost, grad]</span> = costFunction(initial_theta, X, y);</span><br><span class="line"><span class="comment">%  Set options for fminunc</span></span><br><span class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">400</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Run fminunc to obtain the optimal theta</span></span><br><span class="line"><span class="comment">%  This function will return theta and the cost </span></span><br><span class="line"><span class="matrix">[theta, cost]</span> = ...</span><br><span class="line">    fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</span><br></pre></td></tr></table></figure></p>
<p><code>plotDecisionBoundary</code>- - 画出分界线<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plotData(X(:,<span class="number">2</span>:<span class="number">3</span>), y);</span><br><span class="line">hold on</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">size</span>(X, <span class="number">2</span>) &lt;= <span class="number">3</span></span><br><span class="line">    <span class="comment">% Only need 2 points to define a line, so choose two endpoints</span></span><br><span class="line">    plot_x = <span class="matrix">[min(X(:,<span class="number">2</span>))-<span class="number">2</span>,  max(X(:,<span class="number">2</span>))+<span class="number">2</span>]</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% Calculate the decision boundary line</span></span><br><span class="line">    plot_y = (-<span class="number">1.</span>/theta(<span class="number">3</span>)).*(theta(<span class="number">2</span>).*plot_x + theta(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">% Plot, and adjust axes for better viewing</span></span><br><span class="line">    plot(plot_x, plot_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Legend, specific for the exercise</span></span><br><span class="line">    legend(<span class="string">'Admitted'</span>, <span class="string">'Not admitted'</span>, <span class="string">'Decision Boundary'</span>)</span><br><span class="line">    axis(<span class="matrix">[<span class="number">30</span>, <span class="number">100</span>, <span class="number">30</span>, <span class="number">100</span>]</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment">% Here is the grid range</span></span><br><span class="line">    u = <span class="built_in">linspace</span>(-<span class="number">1</span>, <span class="number">1.5</span>, <span class="number">50</span>);</span><br><span class="line">    v = <span class="built_in">linspace</span>(-<span class="number">1</span>, <span class="number">1.5</span>, <span class="number">50</span>);</span><br><span class="line"></span><br><span class="line">    z = <span class="built_in">zeros</span>(<span class="built_in">length</span>(u), <span class="built_in">length</span>(v));</span><br><span class="line">    <span class="comment">% Evaluate z = theta*x over the grid</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(u)</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(v)</span><br><span class="line">            z(<span class="built_in">i</span>,<span class="built_in">j</span>) = mapFeature(u(<span class="built_in">i</span>), v(<span class="built_in">j</span>))*theta;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    z = z<span class="operator">'</span>; <span class="comment">% important to transpose z before calling contour</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Plot z = 0</span></span><br><span class="line">    <span class="comment">% Notice you need to specify the range [0, 0]</span></span><br><span class="line">    contour(u, v, z, <span class="matrix">[<span class="number">0</span>, <span class="number">0</span>]</span>, <span class="string">'LineWidth'</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">hold off</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-30.png" alt=""><br><code>predict</code>- - 根据所得的模型预测新样本<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>); <span class="comment">% Number of training examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to return the following variables correctly</span></span><br><span class="line">p = <span class="built_in">zeros</span>(m, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    <span class="keyword">if</span> sigmoid(X(<span class="built_in">i</span>,:) * theta) &gt;= <span class="number">0.5</span></span><br><span class="line">        p(<span class="built_in">i</span>) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        p(<span class="built_in">i</span>) = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure></p>
<p>预测准确率<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'Train Accuracy: %f\n'</span>, mean(double(p == y)) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure></p>
<h3 id="Regularization-1">Regularization</h3><p>正则化使用的例子是根据一批微晶片两次的测试表现判断它能否通过质量保证（QA）。由于要使用正则化，这次通过<code>mapFeature</code>生成了一个六次的模型。<br><code>plotData</code><br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-31.png" alt=""><br>通过<code>mapFeature</code>将 $X$ 变为如下形式：<br>$$mapFeature(X) = \begin{bmatrix}<br>1\\<br>x_1\\<br>x_2\\<br>x_1^2\\<br>x_1x_2\\<br>x_2^2\\<br>x_1^3\\<br>\vdots \\<br>x_1x_2^5\\<br>x_2^6<br>\end{bmatrix}$$<br>代码为：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">mapFeature</span><span class="params">(X1, X2)</span></span></span><br><span class="line">degree = <span class="number">6</span>;</span><br><span class="line">out = <span class="built_in">ones</span>(<span class="built_in">size</span>(X1(:,<span class="number">1</span>)));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:degree</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">0</span>:<span class="built_in">i</span></span><br><span class="line">        out(:, <span class="keyword">end</span>+<span class="number">1</span>) = (X1.^(<span class="built_in">i</span>-<span class="built_in">j</span>)).*(X2.^<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p><code>costFunctionReg</code>- - 加入了正则化的损失函数<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    h = sigmoid(X(<span class="built_in">i</span>, :) * theta);</span><br><span class="line">    J = J + y(<span class="built_in">i</span>) * <span class="built_in">log</span>(h) + (<span class="number">1</span> - y(<span class="built_in">i</span>)) * <span class="built_in">log</span>(<span class="number">1</span> - h);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">J = (-<span class="number">1</span>/m) * J;</span><br><span class="line">J = J + (theta<span class="operator">'</span> * theta - theta(<span class="number">1</span>)^<span class="number">2</span>) * lambda / (<span class="number">2</span>*m);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 单独更新 theta_0</span></span><br><span class="line">h = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    h = h + (sigmoid(X(<span class="built_in">i</span>, :) * theta) - y(<span class="built_in">i</span>)) * X(<span class="built_in">i</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">grad(<span class="number">1</span>) = h / m;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">2</span>:<span class="built_in">size</span>(X, <span class="number">2</span>)</span><br><span class="line">    h = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">        h = h + (sigmoid(X(<span class="built_in">i</span>, :) * theta) - y(<span class="built_in">i</span>)) * X(<span class="built_in">i</span>, <span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line">    grad(<span class="built_in">j</span>) = h / m + lambda / m * theta(<span class="built_in">j</span>);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure></p>
<p>下面是不同 $\lambda$ 下拟合出来的分界线：<br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-32.png" alt=""><br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-34.png" alt=""><br><img src="http://7xj2nk.com1.z0.glb.clouddn.com/2015-10-26-33.png" alt=""></p>
<p>可以看到 $\lambda$ 设置的特别大的时候，会出现<code>undefit</code>现象。</p>
<hr>
<p>这周看起来内容似乎比较多，但其实难度并不大，下一周要讲神经网络了，这种名词就是那种只听一下就让人感觉云里雾里的玩意，看来得提前开始学习了-_-||</p>
<p>—EOF—</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/10/17/Stanford Machine Learning Week2- -Linear Regression with Multiple Variables/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Stanford Machine Learning Week2- -Linear Regression with Multiple Variables</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>


<div class="share">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">分享到：</span>
		<a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
		<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>



<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="Stanford Machine Learning Week3- -Logistic Regression and Regularization" data-title="Stanford Machine Learning Week3- -Logistic Regression and Regularization" data-url="http://yoursite.com/2015/10/26/Stanford Machine Learning Week3- -Logistic Regression and Regularization/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"grubbyskyer"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>




</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 TIAN Xing
    	</div>
      	<div class="footer-right">
      		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>